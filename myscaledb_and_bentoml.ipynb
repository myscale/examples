{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RAG Application Using MyScale and BentoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) enhances AI applications like chatbots and recommendation systems by combining vector databases and Large Language Models (LLMs). Choosing the right LLM is crucial, considering factors like cost, privacy, and scalability. While commercial LLMs such as OpenAI's GPT-4 and Google's Gemini are effective, they can be expensive and pose data privacy concerns. Open-source LLMs offer flexibility and cost savings but require significant resources for fine-tuning and deployment.\n",
    "\n",
    "A more efficient solution is deploying open-source LLMs on the cloud, providing necessary computational power and scalability without the high costs and complexities of local hosting. This approach reduces initial infrastructural expenses and maintenance concerns. Let's explore developing a RAG application using cloud-hosted open-source LLMs and a scalable vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the required dependencies, enter the following command on your terminal:\n",
    "```bash\n",
    "pip install bentoml langchain clickhouse-connect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "We begin by importing the `WikipediaLoader` from the `langchain_community.document_loaders.wikipedia` module. We use this loader to fetch documents related to \"Albert Einstein\" from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
    "loader = WikipediaLoader(query=\"Albert Einstein\")\n",
    "\n",
    "# Load the documents\n",
    "docs = loader.load()\n",
    "\n",
    "# Display the content of the first document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Text into Chunks\n",
    "We import the `CharacterTextSplitter` from `langchain_text_splitters` to split the document text into manageable chunks. We join the content of all pages into a single string to split the text into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# Split the text into chunks\n",
    "text = ' '.join([page.page_content.replace('\\\\t', ' ') for page in docs])\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\\\n\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([text])\n",
    "splits = [item.page_content for item in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Models on BentoML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready, and the next step is to deploy the models on BentoML and use them in our RAG application. We will deploy the LLM first. For that, you need to create an account on [BentoML](https://cloud.bentoml.com/) if you don’t have any. After that, navigate to the deployments section and click on the \"Create Deployment\" button at the top right corner. A new page will open that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/opening-page.png\" height=\"500\" width=\"1100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Select the \"bentoml/bentovllm-llama3-8b-instruct-service\" model from the drop-down and click \"Submit\" at the bottom right corner. This should start deploying the model and a new page like this will open:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/deploy-the-llm.PNG\" height=\"500\" width=\"1100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment can take some time. Once it is deployed, copy the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: BentoML's free tier only allows the deployment of a single model. If your plan is upgraded and you can deploy more than one model, follow the steps below. If not, don't worry—we will use an open-source model locally for embeddings.\n",
    "\n",
    "The process of deploying the embedding model is very similar to deploying the LLM. You need to follow these steps again:\n",
    "\n",
    "1. Go to the deployments page.\n",
    "2. Click on the \"Create Deployment\" button.\n",
    "3. Select the `sentence-transformers` model from the list and click \"Submit.\"\n",
    "4. Once the deployment is complete, copy the endpoint.\n",
    "\n",
    "Next, go to the API Tokens page and generate a new API key. Now, you are ready to use the deployed models in your RAG application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Embeddings Method\n",
    "We define `get_embeddings` to generate text embeddings. It uses BentoML's service with an endpoint and API token if provided; otherwise, it uses local transformers and `torch` with the `sentence-transformers/all-MiniLM-L6-v2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def get_embeddings(texts: list, BENTO_EMBEDDING_MODEL_END_POINT=None, BENTO_API_TOKEN=None) -> list:\n",
    "    \n",
    "    if BENTO_EMBEDDING_MODEL_END_POINT and BENTO_API_TOKEN:\n",
    "        import bentoml\n",
    "        embedding_client = bentoml.SyncHTTPClient(BENTO_EMBEDDING_MODEL_END_POINT, token=BENTO_API_TOKEN)\n",
    "        return embedding_client.encode(sentences=texts).tolist()\n",
    "    else:\n",
    "        # Install transformers and torch if not already installed\n",
    "        try:\n",
    "            import transformers\n",
    "        except ImportError:\n",
    "            install(\"transformers\")\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            install(\"torch\")\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        \n",
    "        # Initialize the tokenizer and model for embeddings\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings\n",
    "We iterate over the text chunks (splits) in batches of 25 to generate embeddings using the `get_embeddings` function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "for i in range(0, len(splits), 25):\n",
    "    batch = splits[i:i+25]\n",
    "    embeddings_batch = get_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "Now, create a pandas DataFrame to store the text chunks and their corresponding embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'page_content': splits,\n",
    "    'embeddings': all_embeddings\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to MyScaleDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge base is completed and now it’s the time to save the data to the vector database. Here, we are using MyScaleDB for vector storage. To start MyScaleDB cluster on cloud, you can follow the [quickstart guide](https://myscale.com/docs/en/quickstart/#how-to-launch-your-first-cluster). After that, we establish a connection to the MyScaleDB database using the clickhouse_connect library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "client = clickhouse_connect.get_client(\n",
    "    host='your-host-name',\n",
    "    port=443,\n",
    "    username='your-user-name',\n",
    "    password='your-password'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Table and Inserting Data\n",
    "Create a table in MyScaleDB to store the text chunks and embeddings. The table schema includes an ID, the page content, and the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.command(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.RAG (\n",
    "    id Int64,\n",
    "    page_content String,\n",
    "    embeddings Array(Float32),\n",
    "    CONSTRAINT check_data_length CHECK length(embeddings) = 384\n",
    ") ENGINE = MergeTree()\n",
    "    ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the table\n",
    "batch_size = 100\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_data = df[i * batch_size: (i + 1) * batch_size]\n",
    "    client.insert('default.RAG', batch_data.values.tolist(), column_names=batch_data.columns.tolist())\n",
    "    print(f\"Batch {i+1}/{num_batches} inserted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Index\n",
    "The next step is to add a vector index to the embeddings column in the RAG table. The vector index allows for efficient similarity searches, which are essential for retrieval-augmented generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.command(\"\"\"\n",
    "ALTER TABLE default.RAG\n",
    "    ADD VECTOR INDEX vector_index embeddings\n",
    "    TYPE MSTG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Relevant Vectors\n",
    "Let’s define a function to retrieve relevant documents based on a user query. The query embeddings are generated using the `get_embeddings` function, and an advanced SQL vector query is executed to find the closest matches in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs(user_query, top_k):\n",
    "    query_embeddings = get_embeddings(user_query)[0]\n",
    "    results = client.query(f\"\"\"\n",
    "        SELECT page_content,\n",
    "        distance(embeddings, {query_embeddings}) as dist FROM default.RAG ORDER BY dist LIMIT {top_k}\n",
    "    \"\"\")\n",
    "    relevant_docs = \" \"\n",
    "    \n",
    "    for row in results.named_results():\n",
    "        relevant_docs=relevant_docs + row[\"page_content\"]\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Example query\n",
    "message=\"Who is albert einstein?\"\n",
    "relevant_docs = get_relevant_docs(message, 8)\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to BentoML LLM\n",
    "Let’s establish a connection to our hosted LLM on BentoML. The `llm_client` object will be used to interact with the LLM for generating responses based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "BENTO_LLM_END_POINT = \"<https://bentovllm-llama-3-8-b-insruct-service-cffa-88f11f2e.mt-guc1.bentoml.ai>\"\n",
    "\n",
    "llm_client = bentoml.SyncHTTPClient(BENTO_LLM_END_POINT, token=\"your_bento_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing RAG\n",
    "Define a function to perform RAG. The function takes a user question and the retrieved context as input. It constructs a prompt for the LLM, instructing it to answer the question based on the provided context. The response from the LLM is then returned as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dorag(question: str, context: str):\n",
    "    \n",
    "    prompt = (f\"You are a helpful assistant. The user has a question. Answer the user question based only on the context: {context}. \\\\n\"\n",
    "              f\"The user question is {question}\")\n",
    "    \n",
    "    results = llm_client.generate(\n",
    "        max_tokens=1024,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    \n",
    "    res = \"\"\n",
    "    for result in results:\n",
    "        res += result\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Query\n",
    "Finally, we make a query to the RAG application. We ask the question \"Who is Albert Einstein?\" and use the dorag function to get the answer based on the relevant documents retrieved earlier. The output provides a detailed response to the question, demonstrating the effectiveness of the RAG setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is albert einstein?\"\n",
    "dorag(question=query, context=relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the query looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/First-response.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the RAG model was asked about the death of the Alber Einstein, the response looked like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/Second-response.PNG\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
